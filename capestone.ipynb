{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cipri\\\\spark-3.0.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"]='AKIATGK5E2VLLCXL4MWK'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"I94 Data Dictionary\").config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = spark.read.options(delimiter=\"=\", header=None).csv(\"dags/data/source/additional_tables/i94addrl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = df_state.withColumnRenamed(\"_c0\", \"state_code\").withColumnRenamed(\"_c1\", \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_code', 'state']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_state.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code=\"    'AL'\", state=\"'ALABAMA'\"),\n",
       " Row(state_code=\"\\t'AK'\", state=\"'ALASKA'\"),\n",
       " Row(state_code=\"\\t'AZ'\", state=\"'ARIZONA'\"),\n",
       " Row(state_code=\"\\t'AR'\", state=\"'ARKANSAS'\"),\n",
       " Row(state_code=\"\\t'CA'\", state=\"'CALIFORNIA'\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_state.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTabs(dataframe, columns):\n",
    "    for col in columns:\n",
    "        dataframe = dataframe.withColumn(col, F.regexp_replace(dataframe[col], \"\\t\", \"\"))\n",
    "    return dataframe\n",
    "\n",
    "def removeSingleQuotes(dataframe, columns):\n",
    "    for col in columns:\n",
    "        dataframe = dataframe.withColumn(col, F.regexp_replace(dataframe[col], \"'\", \"\"))\n",
    "    return dataframe\n",
    "\n",
    "def trimStrings(dataframe, columns):\n",
    "    for col in columns:\n",
    "        dataframe = dataframe.withColumn(col, F.ltrim(F.rtrim(dataframe[col])))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [\"state_code\", \"state\"]\n",
    "df_state = removeSingleQuotes(df_state, input_data)\n",
    "    # remove tab spaces from state_code\n",
    "df_state = removeTabs(df_state, input_data)\n",
    "    # trim strings\n",
    "df_state = trimStrings(df_state, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='AL', state='ALABAMA'),\n",
       " Row(state_code='AK', state='ALASKA'),\n",
       " Row(state_code='AZ', state='ARIZONA'),\n",
       " Row(state_code='AR', state='ARKANSAS'),\n",
       " Row(state_code='CA', state='CALIFORNIA')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_state.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_state.write.mode(\"overwrite\").parquet(\"s3a://capestone-project-udacity-ciprian/output/\" + \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = spark.read.options(delimiter=\"=\", header=None).csv(\"dags/data/source/additional_tables/i94cntyl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='   582 ', _c1=\"  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\"),\n",
       " Row(_c0='   236 ', _c1=\"  'AFGHANISTAN'\"),\n",
       " Row(_c0='   101 ', _c1=\"  'ALBANIA'\"),\n",
       " Row(_c0='   316 ', _c1=\"  'ALGERIA'\"),\n",
       " Row(_c0='   102 ', _c1=\"  'ANDORRA'\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df_country.withColumnRenamed(\"_c0\", \"country_code\").withColumnRenamed(\"_c1\", \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ['country']\n",
    "df_country = removeSingleQuotes(df_country, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ['country_code', 'country']\n",
    "df_country = removeSingleQuotes(df_country, input_data)\n",
    "df_country = removeTabs(df_country, input_data)\n",
    "df_country = trimStrings(df_country, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr = \"No Country.*\"\n",
    "# df_country = df_country.filter(df_country[\"country\"].rlike(expr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = df_country\\\n",
    "        .withColumn(\"country\",\n",
    "          F.regexp_replace(df_country.country, \n",
    "          \"^INVALID.*|Collapsed.*|No Country.*\", \"INVALID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_country.write.mode(\"overwrite\").parquet(output_data + \"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country_code='582', country='MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'),\n",
       " Row(country_code='236', country='AFGHANISTAN'),\n",
       " Row(country_code='101', country='ALBANIA'),\n",
       " Row(country_code='316', country='ALGERIA'),\n",
       " Row(country_code='102', country='ANDORRA')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = spark.read.options(delimiter=\"=\", header=False).csv(\"dags/data/source/additional_tables/i94prtl.txt\")\n",
    "df_airport = df_airport.withColumnRenamed(\"_c0\", \"port_code\").withColumnRenamed(\"_c1\", \"port_mix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(port_code='   ALC\\t', port_mix='\\tALCAN, AK             '),\n",
       " Row(port_code='   ANC\\t', port_mix='\\tANCHORAGE, AK         '),\n",
       " Row(port_code='   BAR\\t', port_mix='\\tBAKER AAF - BAKER ISLAND, AK'),\n",
       " Row(port_code='   DAC\\t', port_mix='\\tDALTONS CACHE, AK     '),\n",
       " Row(port_code='   PIZ\\t', port_mix='\\tDEW STATION PT LAY DEW, AK')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = ['port_code', 'port_mix']\n",
    "df_airport = removeSingleQuotes(df_airport,input_data)\n",
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = F.split(df_airport.port_mix, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.withColumn(\"city\", split_col.getItem(0))\n",
    "df_airport = df_airport.withColumn(\"state_code\", split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop port_mix\n",
    "df_airport.drop(\"port_mix\")\n",
    "input_data = [\"port_code\",\"city\", \"state_code\"]\n",
    "# remove tab spaces\n",
    "df_airport = removeTabs(df_airport, input_data)\n",
    "# trim spaces\n",
    "df_airport = trimStrings(df_airport, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport[\"port_code\",\"city\", \"state_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(port_code='ALC', city='ALCAN', state_code='AK'),\n",
       " Row(port_code='ANC', city='ANCHORAGE', state_code='AK'),\n",
       " Row(port_code='BAR', city='BAKER AAF - BAKER ISLAND', state_code='AK'),\n",
       " Row(port_code='DAC', city='DALTONS CACHE', state_code='AK'),\n",
       " Row(port_code='PIZ', city='DEW STATION PT LAY DEW', state_code='AK')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state = spark.read.parquet(\"s3a://capestone-project-udacity-ciprian/output/\" + \"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.join(df_state, ['state_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='AK', port_code='ALC', city='ALCAN', state='ALASKA'),\n",
       " Row(state_code='AK', port_code='ANC', city='ANCHORAGE', state='ALASKA'),\n",
       " Row(state_code='AK', port_code='BAR', city='BAKER AAF - BAKER ISLAND', state='ALASKA'),\n",
       " Row(state_code='AK', port_code='DAC', city='DALTONS CACHE', state='ALASKA'),\n",
       " Row(state_code='AK', port_code='PIZ', city='DEW STATION PT LAY DEW', state='ALASKA')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_airport.write.mode(\"overwrite\").parquet(\"s3a://capestone-project-udacity-ciprian/output/\" + \"airport-dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = spark.read.options(delimiter=\"=\", header=False).csv(\"dags/data/source/additional_tables/i94model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='    1 ', _c1=\" 'Air'\"),\n",
       " Row(_c0='\\t2 ', _c1=\" 'Sea'\"),\n",
       " Row(_c0='\\t3 ', _c1=\" 'Land'\"),\n",
       " Row(_c0='\\t9 ', _c1=\" 'Not reported'\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.withColumnRenamed(\"_c0\", \"model_code\").withColumnRenamed(\"_c1\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " input_data = ['model', 'model_code']\n",
    "# remove single quotes\n",
    "df_model = removeSingleQuotes(df_model, input_data)\n",
    "# remove tab spaces\n",
    "input_data = ['model', 'model_code']\n",
    "# remove tab spaces\n",
    "df_model = removeTabs(df_model, input_data)\n",
    "# trim spaces\n",
    "df_model = trimStrings(df_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model_code='1', model='Air'),\n",
       " Row(model_code='2', model='Sea'),\n",
       " Row(model_code='3', model='Land'),\n",
       " Row(model_code='9', model='Not reported')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.write.mode(\"overwrite\").parquet(\"s3a://capestone-project-udacity-ciprian/output/\" + \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa = spark.read.options(delimiter=\"=\", header=False).csv(\"dags/data/source/additional_tables/i94visa.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='   1 ', _c1=' Business'),\n",
       " Row(_c0='   2 ', _c1=' Pleasure'),\n",
       " Row(_c0='   3 ', _c1=' Student')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa = df_visa.withColumnRenamed(\"_c0\", \"visa_code\").withColumnRenamed(\"_c1\", \"visa\")\n",
    "input_data = ['visa_code', 'visa']\n",
    "# remove single quotes\n",
    "df_visa = removeSingleQuotes(df_visa, input_data)\n",
    "# remove tab spaces\n",
    "df_visa = removeTabs(df_visa, input_data)\n",
    "# trim spaces\n",
    "df_visa = trimStrings(df_visa, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(visa_code='1', visa='Business'),\n",
       " Row(visa_code='2', visa='Pleasure'),\n",
       " Row(visa_code='3', visa='Student')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_visa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa.write.mode(\"overwrite\").parquet(\"s3a://capestone-project-udacity-ciprian/output/\" + \"visa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "  airportSchema = R([\n",
    "                        Fld(\"airport_id\",Str()),\n",
    "                        Fld(\"type\",Str()),\n",
    "                        Fld(\"name\",Str()),\n",
    "                        Fld(\"elevation_ft\",Str()),\n",
    "                        Fld(\"continent\",Str()),\n",
    "                        Fld(\"iso_country\",Str()),\n",
    "                        Fld(\"iso_region\",Str()),\n",
    "                        Fld(\"municipality\",Str()),\n",
    "                        Fld(\"gps_code\",Str()),\n",
    "                        Fld(\"iata_code\",Str()),\n",
    "                        Fld(\"local_code\",Str()),\n",
    "                        Fld(\"coordinates\",Str())\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = spark.read.options(delimiter=\",\", header=True, schema=airportSchema).csv(\"dags/data/source/airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125'),\n",
       " Row(ident='00AA', type='small_airport', name='Aero B Ranch Airport', elevation_ft='3435', continent='NA', iso_country='US', iso_region='US-KS', municipality='Leoti', gps_code='00AA', iata_code=None, local_code='00AA', coordinates='-101.473911, 38.704022'),\n",
       " Row(ident='00AK', type='small_airport', name='Lowell Field', elevation_ft='450', continent='NA', iso_country='US', iso_region='US-AK', municipality='Anchor Point', gps_code='00AK', iata_code=None, local_code='00AK', coordinates='-151.695999146, 59.94919968'),\n",
       " Row(ident='00AL', type='small_airport', name='Epps Airpark', elevation_ft='820', continent='NA', iso_country='US', iso_region='US-AL', municipality='Harvest', gps_code='00AL', iata_code=None, local_code='00AL', coordinates='-86.77030181884766, 34.86479949951172'),\n",
       " Row(ident='00AR', type='closed', name='Newport Hospital & Clinic Heliport', elevation_ft='237', continent='NA', iso_country='US', iso_region='US-AR', municipality='Newport', gps_code=None, iata_code=None, local_code=None, coordinates='-91.254898, 35.6087')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_airport = df_airport.filter(\"iso_country='US' and type!='closed'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_airport = df_airport.filter(\"iata_code is not null\")\n",
    "from pyspark.sql.functions import udf, regexp_replace,lower, split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.withColumn(\"latitude\", split(col(\"coordinates\"), \",\")[0].cast(Dbl()))\n",
    "df_airport = df_airport.withColumn(\"longitude\", split(col(\"coordinates\"), \",\")[1].cast(Dbl()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.withColumn(\"state\", split(col(\"iso_region\"), \"-\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename fields\n",
    "#df_airport = df_airport.withColumnRenamed(\"municipality\", \"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.createOrReplaceTempView(\"df_airports\")\n",
    "df_airport = spark.sql(\"\"\"\n",
    "       select ident as airport_id, type, name, elevation_ft, iso_country, state, municipality, gps_code, iata_code as airport_code, latitude, longitude\n",
    "          from df_airports\n",
    "          where iata_code is not null\n",
    "          \n",
    "       union\n",
    "       \n",
    "       select ident as airport_id, type, name, elevation_ft, iso_country, state, municipality, gps_code, local_code  as airport_code, latitude, longitude\n",
    "           from df_airports\n",
    "           where local_code is not null\n",
    "                                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(airport_id='KBFF', type='medium_airport', name='Western Neb. Rgnl/William B. Heilig Airport', elevation_ft='3967', iso_country='US', state='NE', municipality='Scottsbluff', gps_code='KBFF', airport_code='BFF', latitude=-103.5960007, longitude=41.87400055),\n",
       " Row(airport_id='KNRS', type='small_airport', name='Naval Outlying Field Imperial Beach (Ream Field)', elevation_ft='24', iso_country='US', state='CA', municipality='Imperial Beach', gps_code='KNRS', airport_code='NRS', latitude=-117.1169968, longitude=32.56669998),\n",
       " Row(airport_id='KSUE', type='small_airport', name='Door County Cherryland Airport', elevation_ft='725', iso_country='US', state='WI', municipality='Sturgeon Bay', gps_code='KSUE', airport_code='SUE', latitude=-87.42150116, longitude=44.84370041),\n",
       " Row(airport_id='00IN', type='heliport', name='St Mary Medical Center Heliport', elevation_ft='634', iso_country='US', state='IN', municipality='Hobart', gps_code='00IN', airport_code='00IN', latitude=-87.2605972290039, longitude=41.51139831542969),\n",
       " Row(airport_id='02KS', type='small_airport', name='Jmj Landing Airport', elevation_ft='1170', iso_country='US', state='KS', municipality='St Marys', gps_code='02KS', airport_code='02KS', latitude=-96.0552978515625, longitude=39.222198486328125)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_us_airports = spark.read.parquet(\"s3a://capestone-project-udacity-ciprian/output/airport-dict/\")\n",
    "# df_immigration_airport = df_airport.join(df_us_airports, df_us_airports.port_code==df_airport.airport_code, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(airport_id='KBFF', type='medium_airport', name='Western Neb. Rgnl/William B. Heilig Airport', elevation_ft='3967', iso_country='US', state='NE', municipality='Scottsbluff', gps_code='KBFF', airport_code='BFF', latitude=-103.5960007, longitude=41.87400055),\n",
       " Row(airport_id='KNRS', type='small_airport', name='Naval Outlying Field Imperial Beach (Ream Field)', elevation_ft='24', iso_country='US', state='CA', municipality='Imperial Beach', gps_code='KNRS', airport_code='NRS', latitude=-117.1169968, longitude=32.56669998),\n",
       " Row(airport_id='KSUE', type='small_airport', name='Door County Cherryland Airport', elevation_ft='725', iso_country='US', state='WI', municipality='Sturgeon Bay', gps_code='KSUE', airport_code='SUE', latitude=-87.42150116, longitude=44.84370041),\n",
       " Row(airport_id='00IN', type='heliport', name='St Mary Medical Center Heliport', elevation_ft='634', iso_country='US', state='IN', municipality='Hobart', gps_code='00IN', airport_code='00IN', latitude=-87.2605972290039, longitude=41.51139831542969),\n",
       " Row(airport_id='02KS', type='small_airport', name='Jmj Landing Airport', elevation_ft='1170', iso_country='US', state='KS', municipality='St Marys', gps_code='02KS', airport_code='02KS', latitude=-96.0552978515625, longitude=39.222198486328125)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_airport.write.mode(\"overwrite\").partitionBy(\"airport_code\", \"state\").parquet(\"s3a://capestone-project-udacity-ciprian/output/airport-codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics = spark.read.options(delimiter=\";\", header=True, encoding=\"UTF-8\").csv(\"dags/data/source/us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_clean = df_demographics.filter(df_demographics.State.isNotNull()).dropDuplicates(subset=['state', 'city', 'race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demographics.withColumnRenamed(\"State Code\", \"state_code\")\\\n",
    "        .withColumnRenamed(\"Median Age\",\"median_age\")\\\n",
    "        .withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"Male Population\", \"male_population\") \\\n",
    "        .withColumnRenamed(\"Female Population\", \"female_population\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .withColumnRenamed(\"Total Population\", \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.select(\"state_code\", \"state\", \"male_population\",  \"female_population\", \"median_age\", \"race\", \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_category_per_state(spark, dataframe, column):\n",
    "    dataframe.createOrReplaceTempView(\"dataframe\")\n",
    "    dataframe_table = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "        state_code, {0}, count(*) as count_all\n",
    "          FROM dataframe\n",
    "          GROUP BY state_code, {0} \n",
    "    \"\"\".format(column))\n",
    "\n",
    "    dataframe_table.createOrReplaceTempView(\"df_all_table\")\n",
    "\n",
    "    row_number_table = spark.sql(\"\"\"\n",
    "           SELECT\n",
    "            state_code,\n",
    "            {0},\n",
    "            row_number() over (partition by state_code order by count_all desc) as row_number\n",
    "          FROM df_all_table\n",
    "    \"\"\".format(column))\n",
    "\n",
    "    row_number_table.createOrReplaceTempView(\"row_number_table\")\n",
    "\n",
    "    table_final = spark.sql(\"\"\"\n",
    "        SELECT state_code, {0}\n",
    "        FROM row_number_table\n",
    "        WHERE row_number = 1 \n",
    "    \"\"\".format(column))\n",
    "\n",
    "    return table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='AZ', race='Hispanic or Latino'),\n",
       " Row(state_code='SC', race='Hispanic or Latino'),\n",
       " Row(state_code='LA', race='Hispanic or Latino'),\n",
       " Row(state_code='MN', race='White'),\n",
       " Row(state_code='NJ', race='White')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_race=get_first_category_per_state(spark, df_demo, \"race\")\n",
    "df_demo_race.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_race.createOrReplaceTempView(\"df_demographics_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.createOrReplaceTempView(\"df_demographics\")\n",
    "df_demo = spark.sql(\"\"\"\n",
    "                SELECT\n",
    "                    a.state_code, \n",
    "                    state, \n",
    "                    SUM(male_population) as sum_male_population,  \n",
    "                    SUM(female_population) as sum_female_population, \n",
    "                    AVG(CAST(median_age AS DOUBLE)) as median_all_age, \n",
    "                    r.race, \n",
    "                    SUM(CAST(population AS DOUBLE)) as sum_all_population\n",
    "                FROM df_demographics as a\n",
    "                LEFT JOIN df_demographics_race as r on a.state_code=r.state_code\n",
    "                group by a.state_code, a.state, r.race\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='MN', state='Minnesota', sum_male_population=3478803.0, sum_female_population=3565362.0, median_all_age=35.579629629629636, race='White', sum_all_population=7044165.0),\n",
       " Row(state_code='AK', state='Alaska', sum_male_population=764725.0, sum_female_population=728750.0, median_all_age=32.2, race='American Indian and Alaska Native', sum_all_population=1493475.0),\n",
       " Row(state_code='MO', state='Missouri', sum_male_population=3666310.0, sum_female_population=3929660.0, median_all_age=34.86666666666667, race='Asian', sum_all_population=7595970.0),\n",
       " Row(state_code='IA', state='Iowa', sum_male_population=1772066.0, sum_female_population=1831937.0, median_all_age=32.54411764705883, race='White', sum_all_population=3604003.0),\n",
       " Row(state_code='NY', state='New York', sum_male_population=23422799.0, sum_female_population=25579256.0, median_all_age=35.57037037037037, race='Asian', sum_all_population=49002055.0)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_Florida = spark.sql(\"\"\"\n",
    "                SELECT\n",
    "                    state_code, \n",
    "                    state, \n",
    "                    male_population,  \n",
    "                    female_population, \n",
    "                    median_age, \n",
    "                    race, \n",
    "                    population\n",
    "                FROM df_demographics\n",
    "                WHERE state='Florida'\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='FL', state='Florida', male_population='36850', female_population='37165', median_age='37.3', race='White', population='74015'),\n",
       " Row(state_code='FL', state='Florida', male_population='123524', female_population='133564', median_age='41.8', race='White', population='257088'),\n",
       " Row(state_code='FL', state='Florida', male_population='50719', female_population='62480', median_age='34.9', race='Black or African-American', population='113199'),\n",
       " Row(state_code='FL', state='Florida', male_population='55225', female_population='57754', median_age='47.0', race='White', population='112979'),\n",
       " Row(state_code='FL', state='Florida', male_population='55225', female_population='57754', median_age='47.0', race='Asian', population='112979'),\n",
       " Row(state_code='FL', state='Florida', male_population='55225', female_population='57754', median_age='47.0', race='American Indian and Alaska Native', population='112979'),\n",
       " Row(state_code='FL', state='Florida', male_population='89390', female_population='100504', median_age='26.2', race='Hispanic or Latino', population='189894'),\n",
       " Row(state_code='FL', state='Florida', male_population='37155', female_population='42614', median_age='41.4', race='White', population='79769'),\n",
       " Row(state_code='FL', state='Florida', male_population='39504', female_population='45760', median_age='33.5', race='White', population='85264'),\n",
       " Row(state_code='FL', state='Florida', male_population='48090', female_population='44221', median_age='42.5', race='Asian', population='92311'),\n",
       " Row(state_code='FL', state='Florida', male_population='49262', female_population='57520', median_age='39.6', race='White', population='106782'),\n",
       " Row(state_code='FL', state='Florida', male_population='56569', female_population='51202', median_age='42.5', race='Black or African-American', population='107771'),\n",
       " Row(state_code='FL', state='Florida', male_population='66157', female_population='70958', median_age='36.6', race='Black or African-American', population='137115'),\n",
       " Row(state_code='FL', state='Florida', male_population='44210', female_population='45536', median_age='32.2', race='American Indian and Alaska Native', population='89746'),\n",
       " Row(state_code='FL', state='Florida', male_population='32956', female_population='36991', median_age='38.6', race='Black or African-American', population='69947'),\n",
       " Row(state_code='FL', state='Florida', male_population='44760', female_population='48466', median_age='47.3', race='White', population='93226'),\n",
       " Row(state_code='FL', state='Florida', male_population='48754', female_population='52140', median_age='40.8', race='American Indian and Alaska Native', population='100894'),\n",
       " Row(state_code='FL', state='Florida', male_population='84069', female_population='95341', median_age='42.1', race='Asian', population='179410'),\n",
       " Row(state_code='FL', state='Florida', male_population='44853', female_population='43621', median_age='39.9', race='Hispanic or Latino', population='88474'),\n",
       " Row(state_code='FL', state='Florida', male_population='33283', female_population='35869', median_age='36.7', race='Hispanic or Latino', population='69152'),\n",
       " Row(state_code='FL', state='Florida', male_population='32813', female_population='38761', median_age='35.7', race='Hispanic or Latino', population='71574'),\n",
       " Row(state_code='FL', state='Florida', male_population='47840', female_population='56570', median_age='38.1', race='Black or African-American', population='104410'),\n",
       " Row(state_code='FL', state='Florida', male_population='47840', female_population='56570', median_age='38.1', race='Asian', population='104410'),\n",
       " Row(state_code='FL', state='Florida', male_population=None, female_population=None, median_age='70.5', race='Hispanic or Latino', population='72590'),\n",
       " Row(state_code='FL', state='Florida', male_population='55679', female_population='58289', median_age='36.1', race='Asian', population='113968'),\n",
       " Row(state_code='FL', state='Florida', male_population='44760', female_population='48466', median_age='47.3', race='Asian', population='93226'),\n",
       " Row(state_code='FL', state='Florida', male_population='39180', female_population='40956', median_age='43.4', race='American Indian and Alaska Native', population='80136'),\n",
       " Row(state_code='FL', state='Florida', male_population='51767', female_population='56128', median_age='40.7', race='Black or African-American', population='107895'),\n",
       " Row(state_code='FL', state='Florida', male_population='42881', female_population='49674', median_age='40.6', race='Hispanic or Latino', population='92555'),\n",
       " Row(state_code='FL', state='Florida', male_population='75358', female_population='74363', median_age='41.4', race='White', population='149721'),\n",
       " Row(state_code='FL', state='Florida', male_population='48295', female_population='54902', median_age='45.1', race='Hispanic or Latino', population='103197'),\n",
       " Row(state_code='FL', state='Florida', male_population='36658', female_population='39808', median_age='40.2', race='White', population='76466'),\n",
       " Row(state_code='FL', state='Florida', male_population='130940', female_population='139977', median_age='33.1', race='Hispanic or Latino', population='270917'),\n",
       " Row(state_code='FL', state='Florida', male_population='77050', female_population='89574', median_age='40.3', race='Hispanic or Latino', population='166624'),\n",
       " Row(state_code='FL', state='Florida', male_population='37155', female_population='42614', median_age='41.4', race='Black or African-American', population='79769'),\n",
       " Row(state_code='FL', state='Florida', male_population='60803', female_population='69330', median_age='26.0', race='Hispanic or Latino', population='130133'),\n",
       " Row(state_code='FL', state='Florida', male_population='175517', female_population='193511', median_age='35.3', race='Hispanic or Latino', population='369028'),\n",
       " Row(state_code='FL', state='Florida', male_population=None, female_population=None, median_age='70.5', race='Black or African-American', population='72590'),\n",
       " Row(state_code='FL', state='Florida', male_population='33701', female_population='40271', median_age='45.8', race='Hispanic or Latino', population='73972'),\n",
       " Row(state_code='FL', state='Florida', male_population='85626', female_population='89604', median_age='45.0', race='Asian', population='175230'),\n",
       " Row(state_code='FL', state='Florida', male_population='66157', female_population='70958', median_age='36.6', race='White', population='137115'),\n",
       " Row(state_code='FL', state='Florida', male_population='33701', female_population='40271', median_age='45.8', race='White', population='73972'),\n",
       " Row(state_code='FL', state='Florida', male_population='39504', female_population='45760', median_age='33.5', race='Asian', population='85264'),\n",
       " Row(state_code='FL', state='Florida', male_population='44853', female_population='43621', median_age='39.9', race='White', population='88474'),\n",
       " Row(state_code='FL', state='Florida', male_population='48090', female_population='44221', median_age='42.5', race='Black or African-American', population='92311'),\n",
       " Row(state_code='FL', state='Florida', male_population='419203', female_population='448828', median_age='35.7', race='Hispanic or Latino', population='868031'),\n",
       " Row(state_code='FL', state='Florida', male_population='47840', female_population='56570', median_age='38.1', race='American Indian and Alaska Native', population='104410'),\n",
       " Row(state_code='FL', state='Florida', male_population='49262', female_population='57520', median_age='39.6', race='Hispanic or Latino', population='106782'),\n",
       " Row(state_code='FL', state='Florida', male_population='31401', female_population='35099', median_age='31.4', race='Black or African-American', population='66500'),\n",
       " Row(state_code='FL', state='Florida', male_population='38166', female_population='42841', median_age='45.1', race='Asian', population='81007')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo_Florida.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(state_code='MN', state='Minnesota', sum_male_population=3478803.0, sum_female_population=3565362.0, avg(CAST(median_age AS DOUBLE))=35.579629629629636, race='White', sum(CAST(population AS DOUBLE))=7044165.0),\n",
       " Row(state_code='AK', state='Alaska', sum_male_population=764725.0, sum_female_population=728750.0, avg(CAST(median_age AS DOUBLE))=32.2, race='American Indian and Alaska Native', sum(CAST(population AS DOUBLE))=1493475.0),\n",
       " Row(state_code='MO', state='Missouri', sum_male_population=3666310.0, sum_female_population=3929660.0, avg(CAST(median_age AS DOUBLE))=34.86666666666667, race='Asian', sum(CAST(population AS DOUBLE))=7595970.0),\n",
       " Row(state_code='IA', state='Iowa', sum_male_population=1772066.0, sum_female_population=1831937.0, avg(CAST(median_age AS DOUBLE))=32.54411764705883, race='White', sum(CAST(population AS DOUBLE))=3604003.0),\n",
       " Row(state_code='NY', state='New York', sum_male_population=23422799.0, sum_female_population=25579256.0, avg(CAST(median_age AS DOUBLE))=35.57037037037037, race='Asian', sum(CAST(population AS DOUBLE))=49002055.0)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_data = spark.read.options(delimiter=\",\", header=True, encoding=\"UTF-8\").csv(\"dags/data/source/immigration_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='2027561', cicid='4084316.0', i94yr='2016.0', i94mon='4.0', i94cit='209.0', i94res='209.0', i94port='HHW', arrdate='20566.0', i94mode='1.0', i94addr='HI', depdate='20573.0', i94bir='61.0', i94visa='2.0', count='1.0', dtadfile='20160422', visapost=None, occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear='1955.0', dtaddto='07202016', gender='F', insnum=None, airline='JL', admnum='56582674633.0', fltno='00782', visatype='WT')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_date_timedelta = F.udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(int(float(x)))).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_data = df_immigration_data.withColumn(\"arrdate\", udf_date_timedelta(df_immigration_data.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country_code='582', country='MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'),\n",
       " Row(country_code='236', country='AFGHANISTAN'),\n",
       " Row(country_code='101', country='ALBANIA'),\n",
       " Row(country_code='316', country='ALGERIA'),\n",
       " Row(country_code='102', country='ANDORRA')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_data = df_immigration_data['i94yr', 'i94mon', 'i94cit', 'i94res','i94port','arrdate', 'i94mode', 'i94addr','i94bir','i94visa','occup','gender']\n",
    "df_immigration_data = df_immigration_data.withColumnRenamed(\"i94yr\", \"year\")\\\n",
    "        .withColumnRenamed(\"i94mon\", \"month\")\\\n",
    "        .withColumnRenamed(\"i94cit\", \"birth_country\")\\\n",
    "        .withColumnRenamed(\"i94res\", \"residence_country\") \\\n",
    "        .withColumnRenamed(\"i94port\", \"port_code\")\\\n",
    "        .withColumnRenamed(\"i94mode\", \"mode_code\")\\\n",
    "        .withColumnRenamed(\"i94addr\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"i94bir\", \"age\") \\\n",
    "        .withColumnRenamed(\"i94visa\", \"visa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='2016.0', month='4.0', birth_country='209.0', residence_country='209.0', port_code='HHW', arrdate='2016-04-22', mode_code='1.0', state_code='HI', age='61.0', visa='2.0', occup=None, gender='F'),\n",
       " Row(year='2016.0', month='4.0', birth_country='582.0', residence_country='582.0', port_code='MCA', arrdate='2016-04-23', mode_code='1.0', state_code='TX', age='26.0', visa='2.0', occup=None, gender='M'),\n",
       " Row(year='2016.0', month='4.0', birth_country='148.0', residence_country='112.0', port_code='OGG', arrdate='2016-04-07', mode_code='1.0', state_code='FL', age='76.0', visa='2.0', occup=None, gender='M'),\n",
       " Row(year='2016.0', month='4.0', birth_country='297.0', residence_country='297.0', port_code='LOS', arrdate='2016-04-28', mode_code='1.0', state_code='CA', age='25.0', visa='2.0', occup=None, gender='M'),\n",
       " Row(year='2016.0', month='4.0', birth_country='111.0', residence_country='111.0', port_code='CHM', arrdate='2016-04-06', mode_code='3.0', state_code='NY', age='19.0', visa='2.0', occup=None, gender='F')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_data.createOrReplaceTempView(\"df_spark\")\n",
    "df_state.createOrReplaceTempView(\"df_state\")\n",
    "df_visa.createOrReplaceTempView(\"df_visa\")\n",
    "df_model.createOrReplaceTempView(\"df_model\")\n",
    "df_country.createOrReplaceTempView(\"df_country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        count(*) count_all,\n",
    "        sum(case when gender = 'M' then 1 else 0 end) count_man,\n",
    "        sum(case when gender = 'F' then 1 else 0 end) count_woman,\n",
    "        coalesce(c.state_code, '99') as state_code,\n",
    "        i94mon as month\n",
    "    FROM df_spark a\n",
    "    LEFT JOIN df_state c on a.i94addr=c.state_code\n",
    "    group by i94mon, state_code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count_all=2, count_man=1, count_woman=1, state_code='OR', month='4.0'),\n",
       " Row(count_all=13, count_man=7, count_woman=6, state_code='VA', month='4.0'),\n",
       " Row(count_all=1, count_man=0, count_woman=1, state_code='MS', month='4.0'),\n",
       " Row(count_all=1, count_man=0, count_woman=1, state_code='KS', month='4.0'),\n",
       " Row(count_all=1, count_man=1, count_woman=0, state_code='NM', month='4.0'),\n",
       " Row(count_all=163, count_man=86, count_woman=58, state_code='CA', month='4.0'),\n",
       " Row(count_all=5, count_man=3, count_woman=2, state_code='OH', month='4.0'),\n",
       " Row(count_all=1, count_man=1, count_woman=0, state_code='PR', month='4.0'),\n",
       " Row(count_all=10, count_man=6, count_woman=4, state_code='PA', month='4.0'),\n",
       " Row(count_all=161, count_man=59, count_woman=86, state_code='NY', month='4.0'),\n",
       " Row(count_all=1, count_man=1, count_woman=0, state_code='WI', month='4.0'),\n",
       " Row(count_all=1, count_man=1, count_woman=0, state_code='UT', month='4.0'),\n",
       " Row(count_all=27, count_man=20, count_woman=7, state_code='GU', month='4.0'),\n",
       " Row(count_all=12, count_man=6, count_woman=5, state_code='DC', month='4.0'),\n",
       " Row(count_all=12, count_man=1, count_woman=7, state_code='NE', month='4.0'),\n",
       " Row(count_all=1, count_man=0, count_woman=1, state_code='ME', month='4.0'),\n",
       " Row(count_all=2, count_man=0, count_woman=2, state_code='MO', month='4.0'),\n",
       " Row(count_all=31, count_man=14, count_woman=12, state_code='IL', month='4.0'),\n",
       " Row(count_all=26, count_man=5, count_woman=11, state_code='MA', month='4.0'),\n",
       " Row(count_all=19, count_man=15, count_woman=2, state_code='GA', month='4.0')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sex_table.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`count_all`' given input columns: [a._c0, a.admnum, a.airline, a.arrdate, a.biryear, a.cicid, a.count, d.country, d.country_code, a.depdate, a.dtaddto, a.dtadfile, a.entdepa, a.entdepd, a.entdepu, a.fltno, a.gender, a.i94addr, a.i94bir, a.i94cit, a.i94mode, a.i94mon, a.i94port, a.i94res, a.i94visa, a.i94yr, a.insnum, a.matflag, a.occup, c.state, c.state_code, a.visapost, a.visatype]; line 10 pos 10;\n'Aggregate ['a.i94mon, 'a.i94addr, 'a.i94res], ['COUNT('i94res) AS count_all#1535, 'i94res, 'i94addr, 'i94mon AS month#1536]\n+- 'Filter ('count_all = scalar-subquery#1534 [])\n   :  +- 'Aggregate [outer(i94mon#851), outer(i94res#853)], [row_number() windowspecdefinition(i94addr#857, 'count_all DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#1533]\n   :     +- SubqueryAlias b\n   :        +- SubqueryAlias df_spark\n   :           +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, arrdate#970, i94mode#856, i94addr#857, <lambda>(depdate#858) AS depdate#1001, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n   :              +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, <lambda>(arrdate#855) AS arrdate#970, i94mode#856, i94addr#857, depdate#858, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n   :                 +- Relation[_c0#848,cicid#849,i94yr#850,i94mon#851,i94cit#852,i94res#853,i94port#854,arrdate#855,i94mode#856,i94addr#857,depdate#858,i94bir#859,i94visa#860,count#861,dtadfile#862,visapost#863,occup#864,entdepa#865,entdepd#866,entdepu#867,matflag#868,biryear#869,dtaddto#870,gender#871,... 5 more fields] csv\n   +- Join LeftOuter, (i94res#853 = country#97)\n      :- Join LeftOuter, (i94addr#857 = state_code#190)\n      :  :- SubqueryAlias a\n      :  :  +- SubqueryAlias df_spark\n      :  :     +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, arrdate#970, i94mode#856, i94addr#857, <lambda>(depdate#858) AS depdate#1001, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n      :  :        +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, <lambda>(arrdate#855) AS arrdate#970, i94mode#856, i94addr#857, depdate#858, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n      :  :           +- Relation[_c0#848,cicid#849,i94yr#850,i94mon#851,i94cit#852,i94res#853,i94port#854,arrdate#855,i94mode#856,i94addr#857,depdate#858,i94bir#859,i94visa#860,count#861,dtadfile#862,visapost#863,occup#864,entdepa#865,entdepd#866,entdepu#867,matflag#868,biryear#869,dtaddto#870,gender#871,... 5 more fields] csv\n      :  +- SubqueryAlias c\n      :     +- SubqueryAlias df_state\n      :        +- Relation[state_code#190,state#191] parquet\n      +- SubqueryAlias d\n         +- SubqueryAlias df_country\n            +- Project [country_code#91, regexp_replace(country#94, ^INVALID.*|Collapsed.*|No Country.*, INVALID) AS country#97]\n               +- Project [country_code#91, ltrim(rtrim(country#88, None), None) AS country#94]\n                  +- Project [ltrim(rtrim(country_code#85, None), None) AS country_code#91, country#88]\n                     +- Project [country_code#85, regexp_replace(country#82, \t, ) AS country#88]\n                        +- Project [regexp_replace(country_code#79, \t, ) AS country_code#85, country#82]\n                           +- Project [country_code#79, regexp_replace(country#76, ', ) AS country#82]\n                              +- Project [regexp_replace(country_code#70, ', ) AS country_code#79, country#76]\n                                 +- Project [country_code#70, regexp_replace(country#73, ', ) AS country#76]\n                                    +- Project [country_code#70, _c1#65 AS country#73]\n                                       +- Project [_c0#64 AS country_code#70, _c1#65]\n                                          +- Relation[_c0#64,_c1#65] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-1fac5c4b42b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m residence_table = spark.sql(\"\"\"\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mSELECT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mCOUNT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi94res\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcount_all\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mi94res\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi94addr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`count_all`' given input columns: [a._c0, a.admnum, a.airline, a.arrdate, a.biryear, a.cicid, a.count, d.country, d.country_code, a.depdate, a.dtaddto, a.dtadfile, a.entdepa, a.entdepd, a.entdepu, a.fltno, a.gender, a.i94addr, a.i94bir, a.i94cit, a.i94mode, a.i94mon, a.i94port, a.i94res, a.i94visa, a.i94yr, a.insnum, a.matflag, a.occup, c.state, c.state_code, a.visapost, a.visatype]; line 10 pos 10;\n'Aggregate ['a.i94mon, 'a.i94addr, 'a.i94res], ['COUNT('i94res) AS count_all#1535, 'i94res, 'i94addr, 'i94mon AS month#1536]\n+- 'Filter ('count_all = scalar-subquery#1534 [])\n   :  +- 'Aggregate [outer(i94mon#851), outer(i94res#853)], [row_number() windowspecdefinition(i94addr#857, 'count_all DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#1533]\n   :     +- SubqueryAlias b\n   :        +- SubqueryAlias df_spark\n   :           +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, arrdate#970, i94mode#856, i94addr#857, <lambda>(depdate#858) AS depdate#1001, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n   :              +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, <lambda>(arrdate#855) AS arrdate#970, i94mode#856, i94addr#857, depdate#858, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n   :                 +- Relation[_c0#848,cicid#849,i94yr#850,i94mon#851,i94cit#852,i94res#853,i94port#854,arrdate#855,i94mode#856,i94addr#857,depdate#858,i94bir#859,i94visa#860,count#861,dtadfile#862,visapost#863,occup#864,entdepa#865,entdepd#866,entdepu#867,matflag#868,biryear#869,dtaddto#870,gender#871,... 5 more fields] csv\n   +- Join LeftOuter, (i94res#853 = country#97)\n      :- Join LeftOuter, (i94addr#857 = state_code#190)\n      :  :- SubqueryAlias a\n      :  :  +- SubqueryAlias df_spark\n      :  :     +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, arrdate#970, i94mode#856, i94addr#857, <lambda>(depdate#858) AS depdate#1001, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n      :  :        +- Project [_c0#848, cicid#849, i94yr#850, i94mon#851, i94cit#852, i94res#853, i94port#854, <lambda>(arrdate#855) AS arrdate#970, i94mode#856, i94addr#857, depdate#858, i94bir#859, i94visa#860, count#861, dtadfile#862, visapost#863, occup#864, entdepa#865, entdepd#866, entdepu#867, matflag#868, biryear#869, dtaddto#870, gender#871, ... 5 more fields]\n      :  :           +- Relation[_c0#848,cicid#849,i94yr#850,i94mon#851,i94cit#852,i94res#853,i94port#854,arrdate#855,i94mode#856,i94addr#857,depdate#858,i94bir#859,i94visa#860,count#861,dtadfile#862,visapost#863,occup#864,entdepa#865,entdepd#866,entdepu#867,matflag#868,biryear#869,dtaddto#870,gender#871,... 5 more fields] csv\n      :  +- SubqueryAlias c\n      :     +- SubqueryAlias df_state\n      :        +- Relation[state_code#190,state#191] parquet\n      +- SubqueryAlias d\n         +- SubqueryAlias df_country\n            +- Project [country_code#91, regexp_replace(country#94, ^INVALID.*|Collapsed.*|No Country.*, INVALID) AS country#97]\n               +- Project [country_code#91, ltrim(rtrim(country#88, None), None) AS country#94]\n                  +- Project [ltrim(rtrim(country_code#85, None), None) AS country_code#91, country#88]\n                     +- Project [country_code#85, regexp_replace(country#82, \t, ) AS country#88]\n                        +- Project [regexp_replace(country_code#79, \t, ) AS country_code#85, country#82]\n                           +- Project [country_code#79, regexp_replace(country#76, ', ) AS country#82]\n                              +- Project [regexp_replace(country_code#70, ', ) AS country_code#79, country#76]\n                                 +- Project [country_code#70, regexp_replace(country#73, ', ) AS country#76]\n                                    +- Project [country_code#70, _c1#65 AS country#73]\n                                       +- Project [_c0#64 AS country_code#70, _c1#65]\n                                          +- Relation[_c0#64,_c1#65] csv\n"
     ]
    }
   ],
   "source": [
    "# residence_table = spark.sql(\"\"\"\n",
    "#     SELECT\n",
    "#         COUNT(i94res) as count_all,\n",
    "#         i94res,\n",
    "#         i94addr,\n",
    "#         i94mon as month\n",
    "#     FROM df_spark a\n",
    "#     LEFT JOIN df_state c on a.i94addr=c.state_code\n",
    "#     LEFT JOIN df_country d on a.i94res=d.country\n",
    "#     WHERE count_all=(\n",
    "#         SELECT row_number() over (partition by i94addr order by count_all desc) as row_number\n",
    "#         FROM df_spark b\n",
    "#         group by a.i94mon, a.i94res\n",
    "#     )\n",
    "#     group by a.i94mon, a.i94addr, a.i94res  \n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HAVING sum_country=(SELECT MAX(count_country)\n",
    "#                             FROM (SELECT COUNT(i94res) as count_country\n",
    "#                                  FROM df_spark b\n",
    "#                                  WHERE b.i94addr = a.i94addr\n",
    "#                                  group by i94mon, i94addr, i94res) t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(i94addr='AZ', i94res='260.0'),\n",
       " Row(i94addr='SC', i94res='117.0'),\n",
       " Row(i94addr='LA', i94res='438.0'),\n",
       " Row(i94addr='MN', i94res='273.0'),\n",
       " Row(i94addr='NJ', i94res='213.0'),\n",
       " Row(i94addr='DC', i94res='135.0'),\n",
       " Row(i94addr='OR', i94res='260.0'),\n",
       " Row(i94addr='SW', i94res='135.0'),\n",
       " Row(i94addr=None, i94res='209.0'),\n",
       " Row(i94addr='VA', i94res='135.0'),\n",
       " Row(i94addr='VQ', i94res='525.0'),\n",
       " Row(i94addr='RI', i94res='131.0'),\n",
       " Row(i94addr='KY', i94res='112.0'),\n",
       " Row(i94addr='NH', i94res='158.0'),\n",
       " Row(i94addr='MI', i94res='582.0'),\n",
       " Row(i94addr='NV', i94res='582.0'),\n",
       " Row(i94addr='WI', i94res='582.0'),\n",
       " Row(i94addr='ID', i94res='123.0'),\n",
       " Row(i94addr='CA', i94res='438.0'),\n",
       " Row(i94addr='CT', i94res='135.0'),\n",
       " Row(i94addr='NE', i94res='135.0'),\n",
       " Row(i94addr='NC', i94res='213.0'),\n",
       " Row(i94addr='VT', i94res='135.0'),\n",
       " Row(i94addr='MD', i94res='111.0'),\n",
       " Row(i94addr='MO', i94res='438.0'),\n",
       " Row(i94addr='IL', i94res='135.0'),\n",
       " Row(i94addr='ME', i94res='112.0'),\n",
       " Row(i94addr='GU', i94res='209.0'),\n",
       " Row(i94addr='WA', i94res='207.0'),\n",
       " Row(i94addr='MS', i94res='689.0'),\n",
       " Row(i94addr='AL', i94res='112.0'),\n",
       " Row(i94addr='IN', i94res='245.0'),\n",
       " Row(i94addr='OH', i94res='526.0'),\n",
       " Row(i94addr='TN', i94res='209.0'),\n",
       " Row(i94addr='IA', i94res='368.0'),\n",
       " Row(i94addr='NM', i94res='209.0'),\n",
       " Row(i94addr='PA', i94res='135.0'),\n",
       " Row(i94addr='TE', i94res='131.0'),\n",
       " Row(i94addr='NY', i94res='135.0'),\n",
       " Row(i94addr='UN', i94res='135.0'),\n",
       " Row(i94addr='TX', i94res='245.0'),\n",
       " Row(i94addr='GA', i94res='135.0'),\n",
       " Row(i94addr='MA', i94res='112.0'),\n",
       " Row(i94addr='KS', i94res='691.0'),\n",
       " Row(i94addr='CO', i94res='582.0'),\n",
       " Row(i94addr='FL', i94res='135.0'),\n",
       " Row(i94addr='AR', i94res='261.0'),\n",
       " Row(i94addr='OK', i94res='582.0'),\n",
       " Row(i94addr='PR', i94res='518.0'),\n",
       " Row(i94addr='MP', i94res='245.0'),\n",
       " Row(i94addr='UT', i94res='213.0'),\n",
       " Row(i94addr='HI', i94res='209.0')]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# residence_table = spark.sql(\"\"\"\n",
    "#     SELECT\n",
    "#       i94addr, \n",
    "#       i94res, \n",
    "#       count(*) as count_all,\n",
    "#       row_number() over (partition by i94addr order by count_all desc) as row_number\n",
    "#       FROM df_spark  \n",
    "# \"\"\")\n",
    "residence_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    i94addr, {0}, count(*) as count_all\n",
    "      FROM df_spark\n",
    "      GROUP BY i94addr, {0} \n",
    "\"\"\".format(\"i94res\"))\n",
    "\n",
    "\n",
    "residence_table.createOrReplaceTempView(\"df_residence_table\")\n",
    "\n",
    "row_number_residence = spark.sql(\"\"\"\n",
    "       SELECT\n",
    "        i94addr,\n",
    "        {0},\n",
    "        row_number() over (partition by i94addr order by count_all desc) as row_number\n",
    "      FROM df_residence_table\n",
    "\"\"\".format(\"i94res\"))\n",
    "\n",
    "row_number_residence.createOrReplaceTempView(\"row_number_residence\")\n",
    "\n",
    "residence_table_final = spark.sql(\"\"\"\n",
    "    SELECT i94addr, {0}\n",
    "    FROM row_number_residence\n",
    "    WHERE row_number = 1 \n",
    "\"\"\".format(\"i94res\"))\n",
    "\n",
    "residence_table_final.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.createOrReplaceTempView(\"df_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(i94addr='AZ', i94port='LOS'),\n",
       " Row(i94addr='SC', i94port='CLT'),\n",
       " Row(i94addr='LA', i94port='LOS'),\n",
       " Row(i94addr='MN', i94port='SPM'),\n",
       " Row(i94addr='NJ', i94port='NEW'),\n",
       " Row(i94addr='DC', i94port='WAS'),\n",
       " Row(i94addr='OR', i94port='BLA'),\n",
       " Row(i94addr='SW', i94port='SFR'),\n",
       " Row(i94addr=None, i94port='MIA'),\n",
       " Row(i94addr='VA', i94port='WAS'),\n",
       " Row(i94addr='VQ', i94port='X96'),\n",
       " Row(i94addr='RI', i94port='NYC'),\n",
       " Row(i94addr='KY', i94port='CLT'),\n",
       " Row(i94addr='NH', i94port='NYC'),\n",
       " Row(i94addr='MI', i94port='DET'),\n",
       " Row(i94addr='NV', i94port='LVG'),\n",
       " Row(i94addr='WI', i94port='ATL'),\n",
       " Row(i94addr='ID', i94port='SLC'),\n",
       " Row(i94addr='CA', i94port='LOS'),\n",
       " Row(i94addr='CT', i94port='NYC'),\n",
       " Row(i94addr='NE', i94port='NYC'),\n",
       " Row(i94addr='NC', i94port='NYC'),\n",
       " Row(i94addr='VT', i94port='ATL'),\n",
       " Row(i94addr='MD', i94port='WAS'),\n",
       " Row(i94addr='MO', i94port='SFR'),\n",
       " Row(i94addr='IL', i94port='CHI'),\n",
       " Row(i94addr='ME', i94port='TAM'),\n",
       " Row(i94addr='GU', i94port='AGA'),\n",
       " Row(i94addr='WA', i94port='SEA'),\n",
       " Row(i94addr='MS', i94port='MIA'),\n",
       " Row(i94addr='AL', i94port='MIA'),\n",
       " Row(i94addr='IN', i94port='NEW'),\n",
       " Row(i94addr='OH', i94port='MIA'),\n",
       " Row(i94addr='TN', i94port='CHI'),\n",
       " Row(i94addr='IA', i94port='CHI'),\n",
       " Row(i94addr='NM', i94port='DAL'),\n",
       " Row(i94addr='PA', i94port='NEW'),\n",
       " Row(i94addr='TE', i94port='NYC'),\n",
       " Row(i94addr='NY', i94port='NYC'),\n",
       " Row(i94addr='UN', i94port='SFR'),\n",
       " Row(i94addr='TX', i94port='HOU'),\n",
       " Row(i94addr='GA', i94port='ATL'),\n",
       " Row(i94addr='MA', i94port='BOS'),\n",
       " Row(i94addr='KS', i94port='LOS'),\n",
       " Row(i94addr='CO', i94port='PHI'),\n",
       " Row(i94addr='FL', i94port='MIA'),\n",
       " Row(i94addr='AR', i94port='DET'),\n",
       " Row(i94addr='OK', i94port='PHO'),\n",
       " Row(i94addr='PR', i94port='CLM'),\n",
       " Row(i94addr='MP', i94port='SAI'),\n",
       " Row(i94addr='UT', i94port='LOS'),\n",
       " Row(i94addr='HI', i94port='HHW')]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    i94addr, i94port, count(*) as count_all\n",
    "      FROM df_spark\n",
    "      GROUP BY i94addr, i94port \n",
    "\"\"\")\n",
    "\n",
    "\n",
    "airport_table.createOrReplaceTempView(\"df_airport_table\")\n",
    "\n",
    "row_number_airport = spark.sql(\"\"\"\n",
    "       SELECT\n",
    "        i94addr,\n",
    "        i94port,\n",
    "        row_number() over (partition by i94addr order by count_all desc) as row_number\n",
    "      FROM df_airport_table\n",
    "\"\"\")\n",
    "\n",
    "row_number_airport.createOrReplaceTempView(\"row_number_airport\")\n",
    "\n",
    "airport_table_final = spark.sql(\"\"\"\n",
    "    SELECT i94addr, i94port\n",
    "    FROM row_number_airport\n",
    "    WHERE row_number = 1 \n",
    "\"\"\")\n",
    "\n",
    "airport_table_final.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_clean = spark.sql(\"\"\"\n",
    "                                                select \n",
    "                                                        i.i94yr as year,\n",
    "                                                        i.i94mon as month,\n",
    "                                                        i.i94cit as birth_country,\n",
    "                                                        i.i94res as residence_country,\n",
    "                                                        i.i94port as port,\n",
    "                                                        i.arrdate as arrival_date,\n",
    "                                                        coalesce(m.model, 'Not reported') as arrival_mode,\n",
    "                                                        coalesce(c.state_code, '99') as us_state,\n",
    "                                                        i.depdate as departure_date,\n",
    "                                                        i.i94bir as age,\n",
    "                                                        coalesce(v.visa, 'Other') as visa_type_code,\n",
    "                                                        i.dtadfile as date_added,\n",
    "                                                        i.visapost as visa_issued_department,\n",
    "                                                        i.occup as occupation,\n",
    "                                                        i.entdepa as arrival_flag,\n",
    "                                                        i.entdepd as departure_flag,\n",
    "                                                        i.entdepu as update_flag,\n",
    "                                                        i.matflag as match_arrival_departure_fag,\n",
    "                                                        i.biryear as birth_year,\n",
    "                                                        i.dtaddto as allowed_date,\n",
    "                                                        i.insnum as ins_number,\n",
    "                                                        i.airline as airline,\n",
    "                                                        i.admnum as admission_number,\n",
    "                                                        i.visatype as visa_type\n",
    "                                                    from df_spark i left join df_state c on i.i94addr=c.state_code\n",
    "                                                        left join df_visa v on i.i94visa=v.visa_code\n",
    "                                                        left join df_model m on i.i94mode=m.model_code\n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+-------------+--------------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "|  year|month|birth_country|residence_country|port|arrival_date|arrival_mode|us_state|departure_date|repondent_age|visa_type_code|date_added|visa_issued_department|occupation|arrival_flag|departure_flag|update_flag|match_arrival_departure_fag|birth_year|allowed_date|ins_number|airline|admission_number|flight_number|visa_type|\n",
      "+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+-------------+--------------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "|2016.0|  4.0|        209.0|            209.0| HHW|  2016-04-22|Not reported|      HI|    2016-04-29|         61.0|         Other|  20160422|                  null|      null|           G|             O|       null|                          M|    1955.0|    07202016|      null|     JL|   56582674633.0|        00782|       WT|\n",
      "|2016.0|  4.0|        582.0|            582.0| MCA|  2016-04-23|Not reported|      TX|    2016-04-24|         26.0|         Other|  20160423|                   MTR|      null|           G|             R|       null|                          M|    1990.0|    10222016|      null|    *GA|   94361995930.0|        XBLNG|       B2|\n",
      "|2016.0|  4.0|        148.0|            112.0| OGG|  2016-04-07|Not reported|      FL|    2016-04-27|         76.0|         Other|  20160407|                  null|      null|           G|             O|       null|                          M|    1940.0|    07052016|      null|     LH|   55780468433.0|        00464|       WT|\n",
      "|2016.0|  4.0|        297.0|            297.0| LOS|  2016-04-28|Not reported|      CA|    2016-05-07|         25.0|         Other|  20160428|                   DOH|      null|           G|             O|       null|                          M|    1991.0|    10272016|      null|     QR|   94789696030.0|        00739|       B2|\n",
      "|2016.0|  4.0|        111.0|            111.0| CHM|  2016-04-06|Not reported|      NY|    2016-04-09|         19.0|         Other|  20160406|                  null|      null|           Z|             K|       null|                          M|    1997.0|    07042016|      null|   null|   42322572633.0|         LAND|       WT|\n",
      "+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+-------------+--------------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(model_code='1', model='Air'),\n",
       " Row(model_code='2', model='Sea'),\n",
       " Row(model_code='3', model='Land'),\n",
       " Row(model_code='9', model='Not reported')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
